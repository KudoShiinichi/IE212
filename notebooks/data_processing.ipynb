{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lấy 50 bình luận từ 1 trang...\n",
      "Đã lấy 100 bình luận từ 2 trang...\n",
      "Đã lấy 150 bình luận từ 3 trang...\n",
      "Đã lấy đủ 3 trang bình luận.\n",
      "Dữ liệu đã được lưu vào D:\\Kì 5\\BigData\\Final_project\\data\\shoppe_feedback_raw.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "def fetch_shopee_comments(item_id, shop_id, limit=10, offset=0):\n",
    "    \"\"\"\n",
    "    Lấy dữ liệu bình luận từ API của Shopee\n",
    "    \"\"\"\n",
    "    url = \"https://shopee.vn/api/v4/item/get_ratings\"\n",
    "    params = {\n",
    "        \"itemid\": item_id,\n",
    "        \"shopid\": shop_id,\n",
    "        \"limit\": limit,\n",
    "        \"offset\": offset,\n",
    "        \"filter\": 0,  # 0: tất cả, 1: có hình ảnh, 2: 5 sao, 3: 4 sao, ...\n",
    "        \"type\": 0     # 0: bình thường\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        comments = data.get(\"data\", {}).get(\"ratings\", [])\n",
    "        return comments\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def save_comments_to_csv(comments, file_path=\"shopee_comments.csv\"):\n",
    "    \"\"\"\n",
    "    Lưu danh sách bình luận vào file CSV với đường dẫn tùy chỉnh\n",
    "    \"\"\"\n",
    "    with open(file_path, mode=\"w\", newline=\"\", encoding=\"utf-8-sig\") as file:\n",
    "        writer = csv.writer(file, delimiter=';')  # Dùng ';' để tương thích với Excel\n",
    "        # Ghi header\n",
    "        writer.writerow([\"Rating\", \"Comment\", \"Order ID\"])\n",
    "        \n",
    "        # Ghi dữ liệu\n",
    "        for comment in comments:\n",
    "            if not isinstance(comment, dict):\n",
    "                print(f\"Warning: Unexpected data format: {comment}\")\n",
    "                continue  # Bỏ qua nếu dữ liệu không phải là từ điển\n",
    "            \n",
    "            rating = comment.get(\"rating_star\", \"N/A\")\n",
    "            text = comment.get(\"comment\", \"N/A\")\n",
    "            order_id = comment.get(\"orderid\", \"N/A\")\n",
    "\n",
    "            writer.writerow([rating, text, order_id])\n",
    "    print(f\"Dữ liệu đã được lưu vào {file_path}\")\n",
    "\n",
    "def fetch_all_comments(item_id, shop_id, limit=10, max_pages=5):\n",
    "    \"\"\"\n",
    "    Lấy tất cả bình luận bằng cách phân trang, giới hạn số trang\n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    offset = 0\n",
    "    pages_fetched = 0\n",
    "\n",
    "    while True:\n",
    "        if pages_fetched >= max_pages:  # Dừng nếu đã đạt giới hạn số trang\n",
    "            print(f\"Đã lấy đủ {max_pages} trang bình luận.\")\n",
    "            break\n",
    "\n",
    "        comments = fetch_shopee_comments(item_id, shop_id, limit=limit, offset=offset)\n",
    "        if not comments:  # Nếu không còn bình luận, dừng\n",
    "            print(\"Không còn bình luận nào.\")\n",
    "            break\n",
    "\n",
    "        # Kiểm tra dữ liệu trả về từ API\n",
    "        if not isinstance(comments, list):\n",
    "            print(f\"Unexpected response format: {comments}\")\n",
    "            break\n",
    "\n",
    "        all_comments.extend(comments)\n",
    "        offset += limit  # Chuyển sang trang tiếp theo\n",
    "        pages_fetched += 1\n",
    "        print(f\"Đã lấy {len(all_comments)} bình luận từ {pages_fetched} trang...\")\n",
    "\n",
    "    return all_comments\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    # ID sản phẩm và shop từ URL Shopee\n",
    "    item_id = 22088583698  # ID sản phẩm\n",
    "    shop_id = 196261835    # ID shop\n",
    "    \n",
    "    # Giới hạn số trang cần lấy\n",
    "    max_pages = 3  # Ví dụ: Lấy tối đa 3 trang\n",
    "\n",
    "    # Lấy tất cả bình luận\n",
    "    comments = fetch_all_comments(item_id, shop_id, limit=50, max_pages=max_pages)  # Lấy 50 bình luận mỗi trang\n",
    "    \n",
    "    # Tùy chỉnh đường dẫn lưu file CSV\n",
    "    file_path = \"D:\\Kì 5\\BigData\\Final_project\\data\\shoppe_feedback_raw.csv\"\n",
    "    save_comments_to_csv(comments, file_path=file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|Rating|Comment                                                                                                                                 |Order ID|\n",
      "+------+----------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|5     |Tính năng nổi bật:tốt                                                                                                                   |NULL    |\n",
      "|5     |Chất lượng sản phẩm:tốt                                                                                                                 |NULL    |\n",
      "|5     |Tính năng nổi bật:giao hàng đúng màu ship nhiệt tình                                                                                    |NULL    |\n",
      "|5     |Chất lượng sản phẩm:tốt                                                                                                                 |NULL    |\n",
      "|5     |Chất lượng sản phẩm:tuyệt vời Ông Mặt Trời                                                                                              |NULL    |\n",
      "|5     |Tai nghe rẻ mak dùng êm lắm í, sạc nhanh mak dùng lâu ms hết pin. Nghe k bị rè j lun. Chill vc. Shop ship nhanh cực. Nchung nên mua nha.|0       |\n",
      "|5     |Chất lượng sản phẩm:tốt                                                                                                                 |NULL    |\n",
      "|5     |Chất lượng sản phẩm:tốt                                                                                                                 |NULL    |\n",
      "|5     |Chất lượng sản phẩm:tốt                                                                                                                 |NULL    |\n",
      "|5     |Tính năng nổi bật:tai nghe k giây                                                                                                       |NULL    |\n",
      "+------+----------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Error writing processed file: An error occurred while calling o222.csv.\n",
      ": java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim\n",
    "import os\n",
    "\n",
    "def process_csv(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Xử lý file CSV sử dụng PySpark và lưu kết quả\n",
    "    \"\"\"\n",
    "    # Tạo SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Shopee Feedback Processing\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Đọc file CSV\n",
    "    try:\n",
    "        df = spark.read.csv(input_path, header=True, sep=\";\", encoding=\"utf-8\", inferSchema=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        spark.stop()\n",
    "        return\n",
    "\n",
    "    # Xử lý dữ liệu\n",
    "    processed_df = df \\\n",
    "        .withColumn(\"Rating\", col(\"Rating\").cast(\"int\")) \\\n",
    "        .withColumn(\"Comment\", trim(col(\"Comment\"))) \\\n",
    "        .withColumn(\"Order ID\", trim(col(\"Order ID\"))) \\\n",
    "        .dropna(subset=[\"Rating\", \"Comment\"])  # Loại bỏ dòng nếu thiếu Rating hoặc Comment\n",
    "\n",
    "    # Hiển thị dữ liệu sau khi xử lý (tuỳ chọn)\n",
    "    processed_df.show(10, truncate=False)\n",
    "\n",
    "    # Lưu dữ liệu đã xử lý vào file CSV\n",
    "    try:\n",
    "        processed_df.write.csv(output_path, mode=\"overwrite\", header=True, sep=\";\", encoding=\"utf-8\")\n",
    "        print(f\"File đã được xử lý và lưu tại: {os.path.abspath(output_path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing processed file: {e}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    # Đường dẫn file đầu vào (đã loại bỏ BOM)\n",
    "    input_file = \"D:/Kì 5/BigData/Final_project/data/shoppe_feedback_raw.csv\"\n",
    "    # Đường dẫn file đầu ra\n",
    "    output_dir = \"D:/Kì 5/BigData/Final_project/data/processed.csv\"\n",
    "\n",
    "    # Gọi hàm xử lý\n",
    "    process_csv(input_file, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
