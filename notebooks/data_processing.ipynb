{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyvi.ViTokenizer import ViTokenizer\n",
    "import regex as re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import regex as re\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "from pyvi import ViTokenizer, ViPosTagger, ViUtils\n",
    "import joblib\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = '../data/vietnamese_stop_word/vietnamese-stopwords.txt'\n",
    "abb_dict_normal_path = '../data/dictionary/abb_dict_normal.xlsx'\n",
    "abb_dict_special_path = '../data/dictionary/abb_dict_special.xlsx'\n",
    "emoji2word_path = '../data/dictionary/emoji2word.xlsx'\n",
    "character2emoji_path = '../data/dictionary/character2emoji.xlsx'\n",
    "\n",
    "\n",
    "with open(STOPWORDS, \"r\", encoding=\"utf-8\") as ins:\n",
    "    stopwords = []\n",
    "    for line in ins:\n",
    "        dd = line.strip('\\n')\n",
    "        stopwords.append(dd)\n",
    "    stopwords = set(stopwords)\n",
    "\n",
    "def filter_stop_words(train_sentences, stop_words):\n",
    "    new_sent = [word for word in train_sentences.split() if word not in stop_words]\n",
    "    train_sentences = ' '.join(new_sent)\n",
    "    return train_sentences\n",
    "\n",
    "def check_repeated_character(text):\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    count = {}\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] == text[i + 1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_space(text):  # check space in string\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == ' ':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_special_character_numberic(text):\n",
    "    return any(not c.isalpha() for c in text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = str(text).replace(emot, ' ')\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Remove url\n",
    "def url(text):\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', str(text))\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# remove special character\n",
    "def special_character(text):\n",
    "    text = re.sub(r'\\d+', lambda m: \" \", text)\n",
    "    # text = re.sub(r'\\b(\\w+)\\s+\\1\\b',' ', text) #remove duplicate number word\n",
    "    text = re.sub(\"[~!@#$%^&*()_+{}‚Äú‚Äù|:\\\"<>?`¬¥\\-=[\\]\\;\\\\\\/.,]\", \" \", text)\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# normalize repeated characters\n",
    "def repeated_character(text):\n",
    "    text = re.sub(r'(\\w)\\1+', r'\\1', text)\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def mail(text):\n",
    "    text = re.sub(r'[^@]+@[^@]+\\.[^@]+', ' ', text)\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# remove mention tag and hashtag\n",
    "def tag(text):\n",
    "    text = re.sub(r\"(?:\\@|\\#|\\://)\\S+\", \" \", text)\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# \"\"\"Remove all mixed words and numbers\"\"\"\n",
    "def mixed_word_number(text):\n",
    "    text = ' '.join(s for s in text.split() if not any(c.isdigit() for c in s))\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "c2e_path = os.path.join(os.getcwd(), character2emoji_path)\n",
    "character2emoji = pd.read_excel(c2e_path)  # character to emoji\n",
    "def convert_character2emoji(text):\n",
    "    text = str(text)\n",
    "    for i in range(character2emoji.shape[0]):\n",
    "        text = text.replace(character2emoji.at[i, 'character'], \" \" + character2emoji.at[i, 'emoji'] + \" \")\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "e2w_path = os.path.join(os.getcwd(), emoji2word_path)\n",
    "emoji2word = pd.read_excel(e2w_path)  # emoji to word\n",
    "\n",
    "def convert_emoji2word(text):\n",
    "    for i in range(emoji2word.shape[0]):\n",
    "        text = text.replace(emoji2word.at[i, 'emoji'], \" \" + emoji2word.at[i, 'word_vn'] + \" \")\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "adn_path = os.path.join(os.getcwd(), abb_dict_normal_path)\n",
    "abb_dict_normal = pd.read_excel(adn_path)\n",
    "\n",
    "\n",
    "def abbreviation_normal(text):  # len word equal 1\n",
    "    text = str(text)\n",
    "    temp = ''\n",
    "    for word in text.split():\n",
    "        for i in range(abb_dict_normal.shape[0]):\n",
    "            if str(abb_dict_normal.at[i, 'abbreviation']) == str(word):\n",
    "                word = str(abb_dict_normal.at[i, 'meaning'])\n",
    "        temp = temp + ' ' + word\n",
    "    text = temp\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "ads_path = os.path.join(os.getcwd(), abb_dict_special_path)\n",
    "abb_dict_special = pd.read_excel(ads_path)\n",
    "\n",
    "\n",
    "def abbreviation_special(text):  # including special character and number\n",
    "    text = ' ' + str(text) + ' '\n",
    "    for i in range(abb_dict_special.shape[0]):\n",
    "        text = text.replace(' ' + abb_dict_special.at[i, 'abbreviation'] + ' ',\n",
    "                            ' ' + abb_dict_special.at[i, 'meaning'] + ' ')\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def special_character_1(text):  # remove dot and comma\n",
    "    text = re.sub(\"[.,?!]\", \" \", text)\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def abbreviation_kk(text):\n",
    "    text = str(text)\n",
    "    for t in text.split():\n",
    "        if 'kk' in t:\n",
    "            text = text.replace(t, ' ha ha ')\n",
    "        else:\n",
    "            if 'kaka' in t:\n",
    "                text = text.replace(t, ' ha ha ')\n",
    "            else:\n",
    "                if 'kiki' in t:\n",
    "                    text = text.replace(t, ' ha ha ')\n",
    "                else:\n",
    "                    if 'haha' in t:\n",
    "                        text = text.replace(t, ' ha ha ')\n",
    "                    else:\n",
    "                        if 'hihi' in t:\n",
    "                            text = text.replace(t, ' ha ha ')\n",
    "    text = re.sub('  +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_quality_product(text):\n",
    "    # Lo·∫°i b·ªè t·ª´ \"Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m\"\n",
    "    return text.replace(\"Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:\", \"\").strip()\n",
    "\n",
    "def remove_special_function(text):\n",
    "    # Lo·∫°i b·ªè t·ª´ \"T√≠nh nƒÉng n·ªïi b·∫≠t\"\n",
    "    return text.replace(\"T√≠nh nƒÉng n·ªïi b·∫≠t:\", \"\").strip()\n",
    "\n",
    "\n",
    "# tokenize by lib Pyvi\n",
    "def tokenize(text):\n",
    "    text = str(text)\n",
    "    text = ViTokenizer.tokenize(text)\n",
    "    return text\n",
    "\n",
    "def annotations(dataset):\n",
    "    pos = []\n",
    "    max_len = 8000\n",
    "    for i in range(dataset.shape[0]):\n",
    "        n = len(dataset.at[i, 'cmt'])\n",
    "        l = [0] * max_len\n",
    "        s = int(dataset.at[i, 'start_index'])\n",
    "        e = int(dataset.at[i, 'end_index'])\n",
    "        for j in range(s, e):\n",
    "            l[j] = 1\n",
    "        pos.append(l)\n",
    "    return pos\n",
    "\n",
    "def abbreviation_predict(t):\n",
    "    model_path = os.path.join(os.getcwd(), 'abb_model.sav')\n",
    "    loaded_model = joblib.load(model_path)\n",
    "\n",
    "    da_path = os.path.join(os.getcwd(), '../data/dictionary/abbreviation_dictionary_vn.xlsx')\n",
    "    train_path = os.path.join(os.getcwd(), '../data/dictionary/train_duplicate_abb_data.xlsx')\n",
    "    dev_path = os.path.join(os.getcwd(), '../data/dictionary/dev_duplicate_abb_data.xlsx')\n",
    "    test_path = os.path.join(os.getcwd(), '../data/dictionary/test_duplicate_abb_data.xlsx')\n",
    "    duplicate_abb = pd.read_excel(da_path, sheet_name='duplicate', header=None)\n",
    "    duplicate_abb = list(duplicate_abb[0])\n",
    "\n",
    "    train_duplicate_abb_data = pd.read_excel(train_path)\n",
    "    dev_duplicate_abb_data = pd.read_excel(dev_path)\n",
    "    test_duplicate_abb_data = pd.read_excel(test_path)\n",
    "    duplicate_abb_data = pd.concat([train_duplicate_abb_data, dev_duplicate_abb_data, test_duplicate_abb_data],\n",
    "                                   ignore_index=True)\n",
    "    duplicate_abb_data = duplicate_abb_data.drop_duplicates(keep='last').reset_index(drop=True)\n",
    "\n",
    "    X = duplicate_abb_data[['abb', 'start_index', 'end_index', 'cmt']]\n",
    "    y = duplicate_abb_data['origin']\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    enc = DictVectorizer()\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=1200)\n",
    "\n",
    "    temp = annotations(X)\n",
    "    X_pos = sparse.csr_matrix(np.asarray(temp))\n",
    "    X_abb = enc.fit_transform(X[['abb']].to_dict('records'))\n",
    "    X_text = Tfidf_vect.fit_transform(X['cmt'])\n",
    "    X = hstack((X_abb, X_pos, X_text))\n",
    "\n",
    "    text = str(t)\n",
    "    max_len = 8000\n",
    "    if len(t) > max_len:\n",
    "        text = t[:max_len]\n",
    "\n",
    "    cmt = ' ' + text + ' '\n",
    "    for abb in duplicate_abb:\n",
    "        start_index = 0\n",
    "        count = 0\n",
    "        while start_index > -1:  # start_index = -1 -> abb is not in cmt\n",
    "            start_index = cmt.find(' ' + abb + ' ')  # find will return FIRST index abb in cmt\n",
    "            if start_index > -1:\n",
    "                end_index = start_index + len(abb)\n",
    "                t = pd.DataFrame([[abb, start_index, end_index, text]],\n",
    "                                 columns=['abb', 'start_index', 'end_index', 'cmt'], index=None)\n",
    "                temp = annotations(t)\n",
    "                X_pos = sparse.csr_matrix(np.asarray(temp))\n",
    "\n",
    "                X_abb = enc.transform(t[['abb']].to_dict('records'))\n",
    "                # print(t['cmt'])\n",
    "                X_text = Tfidf_vect.transform([text])\n",
    "\n",
    "                X = hstack((X_abb, X_pos, X_text))\n",
    "                predict = loaded_model.predict(X)\n",
    "                origin = le.inverse_transform(predict.argmax(axis=1))\n",
    "                origin = ''.join(origin)\n",
    "                text = text[:start_index + count * (len(origin) - len(abb))] + origin + text[end_index + count * (\n",
    "                            len(origin) - len(abb)):]\n",
    "                text = ''.join(text)\n",
    "                count = count + 1\n",
    "                for i in range(start_index + 1, end_index + 1):  # replace abb to space ' '\n",
    "                    cmt = cmt[:i] + ' ' + cmt[i + 1:]\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, lowercased = False):\n",
    "    text = remove_quality_product(text)\n",
    "    text = remove_special_function(text)\n",
    "    text = filter_stop_words(text, stopwords)\n",
    "    text = text.lower() \n",
    "    text = convert_character2emoji(text)\n",
    "    text = url(text)\n",
    "    text = mail(text)\n",
    "    text = tag(text)\n",
    "    text = mixed_word_number(text)\n",
    "    text = special_character_1(text)  # ##remove , . ? !\n",
    "    text = abbreviation_kk(text)\n",
    "    text = abbreviation_special(text)\n",
    "    text = convert_character2emoji(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = repeated_character(text)\n",
    "    text = special_character(text)\n",
    "    text = abbreviation_normal(text)\n",
    "    # text = abbreviation_predict(text)\n",
    "    text = tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>T√≠nh nƒÉng n·ªïi b·∫≠t:t·ªët\\nCh·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:ch∆∞a dung\\n\\nH√†ng giao nhanh shipbe r·∫•t nhi·ªát t√¨nh m√¨...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-06-02 04:20:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:t·ªët\\n\\nK·∫øt n·ªëi oke nghe ƒëc kh√° ·ªïn.pin ko bt l√† nh∆∞ n√†o m√†u xinh x·ªâu.∆∞ng l·∫Øm ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-08-15 03:47:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>T√≠nh nƒÉng n·ªïi b·∫≠t:giao h√†ng ƒë√∫ng m√†u ship nhi·ªát t√¨nh\\nCh·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:nghe r√µ √¢m thanh chu·∫©n...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-18 09:21:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:tuy·ªát v·ªùi √îng M·∫∑t Tr·ªùi\\nT√≠nh nƒÉng n·ªïi b·∫≠t:ti·ªán d·ª•ng\\n\\nCh·∫•t l∆∞·ª£ng s·∫£n ph·∫©m t...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-06 07:40:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:t·ªët\\nT√≠nh nƒÉng n·ªïi b·∫≠t:ko bt\\n\\nS·∫£n ph·∫©m ƒë·∫πp, ch·∫•t l∆∞·ª£ng t·ªët,ƒëc ƒë√≥ng g√≥i c·∫©n...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-08-05 08:36:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating  \\\n",
       "0       5   \n",
       "1       5   \n",
       "2       5   \n",
       "3       5   \n",
       "4       5   \n",
       "\n",
       "                                                                                               Comment  \\\n",
       "0  T√≠nh nƒÉng n·ªïi b·∫≠t:t·ªët\\nCh·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:ch∆∞a dung\\n\\nH√†ng giao nhanh shipbe r·∫•t nhi·ªát t√¨nh m√¨...   \n",
       "1  Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:t·ªët\\n\\nK·∫øt n·ªëi oke nghe ƒëc kh√° ·ªïn.pin ko bt l√† nh∆∞ n√†o m√†u xinh x·ªâu.∆∞ng l·∫Øm ...   \n",
       "2  T√≠nh nƒÉng n·ªïi b·∫≠t:giao h√†ng ƒë√∫ng m√†u ship nhi·ªát t√¨nh\\nCh·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:nghe r√µ √¢m thanh chu·∫©n...   \n",
       "3  Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:tuy·ªát v·ªùi √îng M·∫∑t Tr·ªùi\\nT√≠nh nƒÉng n·ªïi b·∫≠t:ti·ªán d·ª•ng\\n\\nCh·∫•t l∆∞·ª£ng s·∫£n ph·∫©m t...   \n",
       "4  Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:t·ªët\\nT√≠nh nƒÉng n·ªïi b·∫≠t:ko bt\\n\\nS·∫£n ph·∫©m ƒë·∫πp, ch·∫•t l∆∞·ª£ng t·ªët,ƒëc ƒë√≥ng g√≥i c·∫©n...   \n",
       "\n",
       "   Order ID                 Time  \n",
       "0         0  2024-06-02 04:20:37  \n",
       "1         0  2024-08-15 03:47:50  \n",
       "2         0  2024-03-18 09:21:25  \n",
       "3         0  2024-12-06 07:40:19  \n",
       "4         0  2024-08-05 08:36:14  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ƒê·ªçc file CSV\n",
    "file_path = \"D:\\K√¨ 5\\BigData\\Final_project\\data\\shopee_data_raw.csv\"  # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Ho·∫∑c s·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c `head()` ƒë·ªÉ hi·ªÉn th·ªã 5 d√≤ng ƒë·∫ßu ti√™n\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>processed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>T√≠nh nƒÉng n·ªïi b·∫≠t:t·ªët\\nCh·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:ch∆∞a dung\\n\\nH√†ng giao nhanh shipbe r·∫•t nhi·ªát t√¨nh m√¨...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-06-02 04:20:37</td>\n",
       "      <td>dung h√†ng giao shipbe nhi·ªát_t√¨nh h·ªó_tr·ª£ kh√¥ng bin kh√¥ng gi√° ∆∞ng ·ªßng_h·ªô shop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:t·ªët\\n\\nK·∫øt n·ªëi oke nghe ƒëc kh√° ·ªïn.pin ko bt l√† nh∆∞ n√†o m√†u xinh x·ªâu.∆∞ng l·∫Øm ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-08-15 03:47:50</td>\n",
       "      <td>k·∫øt_n·ªëi ƒë∆∞·ª£c ƒë∆∞·ª£c ·ªïn pin kh√¥ng bt m√†u xinh x·ªâu ∆∞ng l·∫Øm lu√¥n m√† gi√° r·∫ª ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>T√≠nh nƒÉng n·ªïi b·∫≠t:giao h√†ng ƒë√∫ng m√†u ship nhi·ªát t√¨nh\\nCh·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:nghe r√µ √¢m thanh chu·∫©n...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-18 09:21:25</td>\n",
       "      <td>giao h√†ng m√†u ship nhi·ªát_t√¨nh √¢m chu·∫©n h√†ng giao nhanh nghe √™m tai ƒë·ªìng_h·ªì</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:tuy·ªát v·ªùi √îng M·∫∑t Tr·ªùi\\nT√≠nh nƒÉng n·ªïi b·∫≠t:ti·ªán d·ª•ng\\n\\nCh·∫•t l∆∞·ª£ng s·∫£n ph·∫©m t...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-06 07:40:19</td>\n",
       "      <td>tuy·ªát_v·ªùi √¥ng m·∫∑t_tr·ªùi ti·ªán_d·ª•ng ch·∫•t s·∫£ng ph·∫©m tuy·ªát_v·ªùi √¥ng m·∫∑t_tr·ªùi lu√¥n shop bao_g√≥i h√†ng k·ªπ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:t·ªët\\nT√≠nh nƒÉng n·ªïi b·∫≠t:ko bt\\n\\nS·∫£n ph·∫©m ƒë·∫πp, ch·∫•t l∆∞·ª£ng t·ªët,ƒëc ƒë√≥ng g√≥i c·∫©n...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-08-05 08:36:14</td>\n",
       "      <td>kh√¥ng bt s·∫£ng ph·∫©m ƒë·∫πp ch·∫•t t·ªët ƒë∆∞·ª£c ƒë√≥ng_g√≥i c·∫©n_th·∫≠n gi√° h·ª£p_l√Ω shiper th√¢n_thi·ªán m·ªçi ng∆∞·ªùi mu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating  \\\n",
       "0       5   \n",
       "1       5   \n",
       "2       5   \n",
       "3       5   \n",
       "4       5   \n",
       "\n",
       "                                                                                               Comment  \\\n",
       "0  T√≠nh nƒÉng n·ªïi b·∫≠t:t·ªët\\nCh·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:ch∆∞a dung\\n\\nH√†ng giao nhanh shipbe r·∫•t nhi·ªát t√¨nh m√¨...   \n",
       "1  Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:t·ªët\\n\\nK·∫øt n·ªëi oke nghe ƒëc kh√° ·ªïn.pin ko bt l√† nh∆∞ n√†o m√†u xinh x·ªâu.∆∞ng l·∫Øm ...   \n",
       "2  T√≠nh nƒÉng n·ªïi b·∫≠t:giao h√†ng ƒë√∫ng m√†u ship nhi·ªát t√¨nh\\nCh·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:nghe r√µ √¢m thanh chu·∫©n...   \n",
       "3  Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:tuy·ªát v·ªùi √îng M·∫∑t Tr·ªùi\\nT√≠nh nƒÉng n·ªïi b·∫≠t:ti·ªán d·ª•ng\\n\\nCh·∫•t l∆∞·ª£ng s·∫£n ph·∫©m t...   \n",
       "4  Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:t·ªët\\nT√≠nh nƒÉng n·ªïi b·∫≠t:ko bt\\n\\nS·∫£n ph·∫©m ƒë·∫πp, ch·∫•t l∆∞·ª£ng t·ªët,ƒëc ƒë√≥ng g√≥i c·∫©n...   \n",
       "\n",
       "   Order ID                 Time  \\\n",
       "0         0  2024-06-02 04:20:37   \n",
       "1         0  2024-08-15 03:47:50   \n",
       "2         0  2024-03-18 09:21:25   \n",
       "3         0  2024-12-06 07:40:19   \n",
       "4         0  2024-08-05 08:36:14   \n",
       "\n",
       "                                                                                        processed_data  \n",
       "0                          dung h√†ng giao shipbe nhi·ªát_t√¨nh h·ªó_tr·ª£ kh√¥ng bin kh√¥ng gi√° ∆∞ng ·ªßng_h·ªô shop  \n",
       "1            k·∫øt_n·ªëi ƒë∆∞·ª£c ƒë∆∞·ª£c ·ªïn pin kh√¥ng bt m√†u xinh x·ªâu ∆∞ng l·∫Øm lu√¥n m√† gi√° r·∫ª ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞  \n",
       "2                           giao h√†ng m√†u ship nhi·ªát_t√¨nh √¢m chu·∫©n h√†ng giao nhanh nghe √™m tai ƒë·ªìng_h·ªì  \n",
       "3  tuy·ªát_v·ªùi √¥ng m·∫∑t_tr·ªùi ti·ªán_d·ª•ng ch·∫•t s·∫£ng ph·∫©m tuy·ªát_v·ªùi √¥ng m·∫∑t_tr·ªùi lu√¥n shop bao_g√≥i h√†ng k·ªπ...  \n",
       "4  kh√¥ng bt s·∫£ng ph·∫©m ƒë·∫πp ch·∫•t t·ªët ƒë∆∞·ª£c ƒë√≥ng_g√≥i c·∫©n_th·∫≠n gi√° h·ª£p_l√Ω shiper th√¢n_thi·ªán m·ªçi ng∆∞·ªùi mu...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_data'] = df['Comment'].apply(lambda x: preprocessing(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m:t·ªët\\n\\nK·∫øt n·ªëi oke nghe ƒëc kh√° ·ªïn.pin ko bt l√† nh∆∞ n√†o m√†u xinh x·ªâu.∆∞ng l·∫Øm lun.m√† gi√° r·∫ª n·ªØa ü´∞ü´∞ü´∞ü´∞ü´∞ü´∞ü´∞ü´∞ü´∞ü´∞'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Comment'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k·∫øt_n·ªëi ƒë∆∞·ª£c ƒë∆∞·ª£c ·ªïn pin kh√¥ng bt m√†u xinh x·ªâu ∆∞ng l·∫Øm lu√¥n m√† gi√° r·∫ª ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞ ü´∞'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_data'][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
